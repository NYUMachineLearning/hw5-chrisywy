---
title: "WYu HW5"
author: "Wuyue Yu"
date: "11/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tree-Based Methods
```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(dplyr) 
library(ISLR)
library(tree)
library(randomForest)
library(MASS)
library(gbm)
library(rpart)
library(rpart.plot)
library(Metrics)
```

```{r carseats}
data("Carseats")
carseats = Carseats
summary(carseats)
head(carseats)
```

1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results. 

```{r Regression Tree}
#Set seed to make results reproducible 
set.seed(29)

#Split data into train and test subset (250 and 150 respectively)
train = sample(1:nrow(carseats), 250)

#Fit train subset of data to model 
tree.carseats = rpart(Sales ~ ., carseats, subset=train, method = "anova")
printcp(tree.carseats)
summary(tree.carseats)

#Visualize tree
rpart.plot(tree.carseats)

#Use model on test set, predict sales
tree.pred = predict(tree.carseats, carseats[-train,])
plot(carseats[-train,1],tree.pred)
rmse(carseats[-train,1],tree.pred)

```
Carseats dataset chosen to predict sales with other variables.      
Regression tree as plotted.      
Variables used in model: Advertising, Age, CompPrice, Income, Population, Price, ShelveLoc, US     
17 internal nodes, 18 terminal nodes.     
Sales predicted for test set. RMSE is 1.84955; on average, predicted sales are about 1.85 thousand units off from the actual sales.    
           
                
2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.         
##Bagging

```{r Bagging}
set.seed(1)

oob.err = double(10)
test.err = double(10)

#find mtry and ntree 
for(mtry in 1:10){
  fit = randomForest(Sales ~ ., carseats, subset=train, mtry = mtry, ntree = 500)
  oob.err[mtry] = fit$mse[500] ##extract Mean-squared-error 
  pred = predict(fit, carseats[-train,]) #predict on test dataset
  test.err[mtry] = with(carseats[-train,], mean( (Sales-pred)^2 )) #compute test error
}

bagging.carseats = randomForest(Sales~., data = carseats, subset = train, mtry = 10, ntree = 500)
bagging.carseats

#Visualize 
plot(pred,carseats[-train,1], ylab = "Sales", xlim = c(3,16), ylim = c(3,16))
abline(0,1)
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))

```

Random forest uses a random sample of m predicting features each time to fit a tree. Number of predictors and trees are tuned.   
Training and test errors both decrease as number of predictors increases.     
Multiple ntree values attempted. Increasing ntree grows a larger number of trees in random forest, stablizing the model, resulting in a lower mean of squared residuals and a higher percentage of var explained.          
mtry = 10, ntree = 500 selected for model.     

         
##Boosting

```{r Boosting}
set.seed(1)
#Gradient Boosting Model
boost.carseats = gbm(Sales~., data = carseats[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

#Variable Importance Plot
summary(boost.carseats)

#Visualize important variables of interest
plot(boost.carseats,i="ShelveLoc")
plot(boost.carseats,i="Price")

#Predict on test set
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.carseats, newdata = carseats[-train,], n.trees = n.trees)
dim(predmat)

#Visualize Boosting Error Plot
boost.err = with(carseats[-train,], apply( (predmat - Sales)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")

```
     
Boosting uses information from previously grown trees to sequentially train following tree models.     
Relative influences of variables are shown, with importance of Price and ShelveLoc visualized.    
Mean squared error decreases with increasing number of trees, reaches minimum at ~1200 trees, slightly increases again and plateaus out. Therefore n.trees should be set to ~1200.        




